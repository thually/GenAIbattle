from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field

from LIBS.WXlib import WX

app = FastAPI()

# Instantiate the wx object only ONCE in your application.
wx = WX()

# Set the model parameters or model name only if you want to change the model ID or its default parameters.
modelParameters = {
    "decoding_method": "greedy",
    "max_new_tokens": 2048,
    "min_new_tokens": 0,
    "stop_sequences": [ ],
    "repetition_penalty": 1
}
wx.wxInstModel(modelID='meta-llama/llama-3-70b-instruct', modelParams=modelParameters)

# Set the prompt template only once if you want to change the model behavior or expected output.
promptTemplate = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. 
Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.

If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<|eot_id|>

<|start_header_id|>context<|end_header_id|>

You are going to play the game "Who am I?" with the user.
It's a popular guessing game where one person thinks of a character, and the other person tries to guess who it is by asking yes or no questions.
The goal is to figure out the identity of the character without asking too many questions.
You are gonna be Alan Turing.
The user is going to ask you yes or no questions trying to guest who you are (you are gonna be Alan Turing).
DO NOT reveal your character's identity outright.
If the user ask a question that is not a yes or no question, you should respond with "Please ask a yes or no question."
Respond to the user's questions truthfully, but do not give any additional information.
If the user correctly guesses that you are Alan Turing, inform them that they have won and how many questions they asked.<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Welcome to the game "Who am I?"! You can start by asking me a yes or no question to guess who I am.<|eot_id|>"""

promptTemplate_copy = promptTemplate # Save a copy of the prompt template to reset the conversation history

next_prompt = """<|start_header_id|>user<|end_header_id|>

{{QUESTION}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""
promptVariables = {
    'QUESTION' : None
}

# specify the expected structure and data types of the request body. 
# In this case, it expects a JSON object with a single field:
# question: str: A field named question that should be a string. 
class ApiQuestionRequest(BaseModel):
    # This is the only data your endpoint expects in incoming requests.
    question: str = Field(..., 
        example="Hi, how are you?",
        description="In this field, please enter a question in English to be passed to LLM."
    )

class ApiQuestionResponse(BaseModel):
    # This is the only data your endpoint returns in response.
    question: str = Field(..., 
        example="What is the value of a circle's area divided by pi, where the radius of a circle is 2?",
        description="In this field, the content of the original question submitted with the POST /api request will be returned."
    )
    answer: str = Field(..., 
        example="To find the value, you would first calculate the area of the circle , and then divide the result by Pi. The answer is 4.",
        description="In this field, the text generated by the Language Model will be displayed."
    )


# A decorator that creates a route for POST requests to the URL path /question/. 
# function generate_answer defined below is to be called when a POST request to /question/ is received
@app.post("/api/question")
async def apiQuestion(request: ApiQuestionRequest) -> ApiQuestionResponse:
    # Automatically parse the JSON body of incoming requests, validate them against the Question model,
    # and pass a Question instance to the function for access to the request body.

    # TODO: 
    # Begin your code block for LLM interaction.
    global promptTemplate, next_prompt, promptVariables
    prompt = request.question

    if prompt == "reset":
        promptTemplate = promptTemplate_copy
        promptVariables['QUESTION'] = None
        return ApiQuestionResponse(question="reset", answer="The conversation history has been reset.\n\nWelcome to the game 'Who am I?'! You can start by asking me a yes or no question to guess who I am.")

    promptVariables['QUESTION'] = prompt
    generated_text = wx.wxGenText(promptTemplate=promptTemplate + next_prompt, promptVariables=promptVariables)

    # We save the conversation history to 'promptTemplate' to keep the context for the next iteration
    promptTemplate += f"<|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{generated_text}<|eot_id|>"
    # End of your code block

    # Return response
    return ApiQuestionResponse(question=prompt, answer=generated_text)
